{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6saNLUFqHb"
      },
      "source": [
        "# Assignment 2 - CT5120/CT5146\n",
        "\n",
        "### Instructions:\n",
        "- Complete all the tasks below and upload your submission as a Python notebook on Blackboard with the filename “`StudentID_Lastname.ipynb`” before **23:59** on **October 24, 2021**.\n",
        "- This is an individual assignment, you **must not** work with other students to complete this assessment.\n",
        "- The assignment is worth $100$ marks and constitutes 19% of the final grade. The breakdown of the marking scheme for each task is as follows:\n",
        "\n",
        "| Task | Marks for write-up | Marks for code | Total Marks |\n",
        "| :--- | :----------------- | :------------- | :---------- |\n",
        "| 1    |                  - |              5 |           5 |\n",
        "| 2    |                 15 |             15 |          30 |\n",
        "| 3    |                  - |             10 |          10 |\n",
        "| 4    |                 10 |              5 |          15 |\n",
        "| 5    |                 15 |             25 |          40 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCWSEtNeGMsN"
      },
      "source": [
        "---\n",
        "\n",
        "This assignment involves tasks for feature engineering, training and evaluating a classifier for suggestion detection. You will work with the data from SemEval-2019 Task 9 subtask A to classify whether a piece of text contains a suggestion or not. \n",
        "\n",
        "\n",
        "Download `Data.Assignment2.SemEvalTask9SubtaskA.csv` from Blackboard or uncomment the code cell below to get the data as a comma-separated values (CSV) file. The CSV file contains a header row followed by 6,100 rows spread across 3 columns of data. Each row of data contains a unique id, a piece of text and a label assigned by an annotator. A label of $1$ indicates that the given text contains a suggestion while a label of $0$ indicates that the text does not contain a suggestion.\n",
        "\n",
        "You can find more details about the dataset in Sections 1, 2, 3 and 4 of [SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums\n",
        "](https://aclanthology.org/S19-2151/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3PrRwfwGON1"
      },
      "source": [
        "# !curl \"https://raw.githubusercontent.com/pasricha/Subtask-A/master/Data.Assignment2.SemEvalTask9SubtaskA.csv\" > Data.Assignment2.SemEvalTask9SubtaskA.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RohgBTdkHX0Z"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 1: Reading Data (5 marks)\n",
        "\n",
        "The following cell of code reads the texts and the corresponding labels of suggestion/non-suggestion from the CSV file. The first task is to create training and test sets. Use the final $1000$ rows of the data as a test set and the rest of the data for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x0c38rCGk23"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file.\n",
        "df = pd.read_csv('Data.Assignment2.SemEvalTask9SubtaskA.csv', \n",
        "                 names=['id', 'text', 'label'], header=0)\n",
        "\n",
        "# Set seed for reproducibility and shuffle the rows.\n",
        "np.random.seed(888)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Store the data as a list of tuples where the first item is the text\n",
        "# and the second item is the label.\n",
        "data = [(text, label) for (idx, text, label) in df.values.tolist()]\n",
        "\n",
        "# Create training and test sets.\n",
        "train_texts, train_labels = [], []\n",
        "test_texts, test_labels = [], []\n",
        "\n",
        "#################### EDIT BELOW THIS LINE #########################\n",
        "\n",
        "# your code goes here\n",
        "\n",
        "\n",
        "#################### EDIT ABOVE THIS LINE #########################\n",
        "\n",
        "# Check that training set and test set are of the right size.\n",
        "assert len(test_texts) == len(test_labels) == 1000\n",
        "assert len(train_texts) == len(train_labels) == 5100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Scj45oSpdQ"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 2: Data Pre-processing (30 Marks)\n",
        "\n",
        "Explain at least 3 steps that you will perform to preprocess the texts before training a classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROipkmu1cnN_"
      },
      "source": [
        "Edit this cell to write your answer below the line in no more than 300 words.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> Delete this line and write your answer here\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2-xXQggaVKh"
      },
      "source": [
        "In the code cell below, write an implementation of the steps you defined above. You are free to use a library such as `nltk` or `sklearn` for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb7i3Le4aSYM"
      },
      "source": [
        "# your code goes here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IUJunnfXItQ"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 3: Feature Engineering (I) - TF-IDF as features (10 Marks)\n",
        "\n",
        "In the lectures we have seen that raw counts of words and `tf-idf` scores can be useful features for a classification task. Complete the following code cell to create a suggestion detector which uses `tf-idf` scores as features for a Naïve Bayes classifier.\n",
        "\n",
        "After applying your preprocessing steps, use the training data to train the classifier and make predictions on the test set. You **must not** use the test set for training.\n",
        "\n",
        "If everything is implemented correctly, then you should see a single floating point value between 0 and 1 at the end which denotes the accuracy of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gDsfB8xTGMg",
        "collapsed": true
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Calculate tf-idf scores for the words in the training set.\n",
        "# ... your code goes here\n",
        "\n",
        "\n",
        "\n",
        "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
        "# ... your code goes here\n",
        "\n",
        "\n",
        "\n",
        "# Predict on the test set.\n",
        "predictions = []    # save your predictions on the test set into this list\n",
        "\n",
        "# ... your code goes here\n",
        "\n",
        "\n",
        "\n",
        "#################### DO NOT EDIT BELOW THIS LINE #################\n",
        "\n",
        "def accuracy(labels, predictions):\n",
        "  '''\n",
        "  Calculate the accuracy score for a given set of predictions and labels.\n",
        "  \n",
        "  Args:\n",
        "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
        "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
        "\n",
        "  Returns:\n",
        "    float: A floating point value to score the predictions against the labels.\n",
        "  '''\n",
        "\n",
        "  assert len(labels) == len(predictions)\n",
        "  \n",
        "  correct = 0\n",
        "  for label, prediction in zip(labels, predictions):\n",
        "    if label == prediction:\n",
        "      correct += 1 \n",
        "  \n",
        "  score = correct / len(labels)\n",
        "  return score\n",
        "\n",
        "# Calculate accuracy score for the classifier using tf-idf features.\n",
        "accuracy(test_labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDx_M2aTIncl"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 4: Evaluation Metrics (15 marks)\n",
        "\n",
        "Why is accuracy not the best measure for evaluating a classifier? Describe an evaluation metric which might work better than accuracy for a classification task such as suggestion detection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1evdGdZf66Aw"
      },
      "source": [
        "Edit this cell to write your answer below the line in no more than 150 words.\n",
        "\n",
        "---\n",
        "\n",
        "> Delete this line and write your answer here\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ozD4SyyRDL3"
      },
      "source": [
        "In the code cell below, write an implementation of the evaluation metric you defined above. You are free to use a library such as `nltk` or `sklearn` for this task, or you can write your own implementation from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkUX5K0oMhKI"
      },
      "source": [
        "def evaluate(labels, predictions):\n",
        "  '''\n",
        "  Calculate an evaluation score other than accuracy for a given set of predictions and labels.\n",
        "  \n",
        "  Args:\n",
        "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
        "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
        "\n",
        "  Returns:\n",
        "    float: A floating point value to score the predictions against the labels.\n",
        "  '''\n",
        "\n",
        "  # check that labels and predictions are of same length\n",
        "  assert len(labels) == len(predictions)\n",
        "\n",
        "  score = 0.0\n",
        "  \n",
        "  #################### EDIT BELOW THIS LINE #########################\n",
        "\n",
        "  # your code goes here\n",
        "\n",
        "\n",
        "  #################### EDIT ABOVE THIS LINE #########################\n",
        "\n",
        "  return score\n",
        "\n",
        "# Calculate evaluation score based on the metric of your choice\n",
        "# for the classifier trained in Task 3 using tf-idf features.\n",
        "evaluate(test_labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22OelF89a27J"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 5: Feature Engineering (II) - Other features (40 Marks)\n",
        "\n",
        "Describe features other than those defined in Task 3 which might improve the performance of your suggestion detector. If these features require any additional pre-processing steps, then define those steps as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQT0m3vG7bNc"
      },
      "source": [
        "Edit this cell to write your answer below the line in no more than 500 words.\n",
        "\n",
        "---\n",
        "\n",
        "> Delete this line and write your answer here\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHYzUHSW7iPx"
      },
      "source": [
        "In the code cell below, write an implementation of the features (and any additional pre-preprocessing steps) you defined above. You are free to use a library such as `nltk` or `sklearn` for this task.\n",
        "\n",
        "After creating your features, use the training data to train a Naïve Bayes classifier and use the test set to evaluate its performance using the metric defined in Task 4. You **must not** use the test set for training.\n",
        "\n",
        "To make sure that your code doesn't take too long to run or use too much memory, you can consider a time limit of 3 minutes and a memory limit of 12GB for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9mRku0va8kK"
      },
      "source": [
        "# Create your features.\n",
        "# ... your code goes here\n",
        "\n",
        "\n",
        "\n",
        "# Train a Naïve Bayes classifier using the features you defined.\n",
        "# ... your code goes here\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate on the test set.\n",
        "# ... your code goes here\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}