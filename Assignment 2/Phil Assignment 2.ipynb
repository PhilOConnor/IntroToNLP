{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6saNLUFqHb"
   },
   "source": [
    "# Assignment 2 - CT5120/CT5146\n",
    "\n",
    "### Instructions:\n",
    "- Complete all the tasks below and upload your submission as a Python notebook on Blackboard with the filename “`StudentID_Lastname.ipynb`” before **23:59** on **October 24, 2021**.\n",
    "- This is an individual assignment, you **must not** work with other students to complete this assessment.\n",
    "- The assignment is worth $100$ marks and constitutes 19% of the final grade. The breakdown of the marking scheme for each task is as follows:\n",
    "\n",
    "| Task | Marks for write-up | Marks for code | Total Marks |\n",
    "| :--- | :----------------- | :------------- | :---------- |\n",
    "| 1    |                  - |              5 |           5 |\n",
    "| 2    |                 15 |             15 |          30 |\n",
    "| 3    |                  - |             10 |          10 |\n",
    "| 4    |                 10 |              5 |          15 |\n",
    "| 5    |                 15 |             25 |          40 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCWSEtNeGMsN"
   },
   "source": [
    "---\n",
    "\n",
    "This assignment involves tasks for feature engineering, training and evaluating a classifier for suggestion detection. You will work with the data from SemEval-2019 Task 9 subtask A to classify whether a piece of text contains a suggestion or not. \n",
    "\n",
    "\n",
    "Download `Data.Assignment2.SemEvalTask9SubtaskA.csv` from Blackboard or uncomment the code cell below to get the data as a comma-separated values (CSV) file. The CSV file contains a header row followed by 6,100 rows spread across 3 columns of data. Each row of data contains a unique id, a piece of text and a label assigned by an annotator. A label of $1$ indicates that the given text contains a suggestion while a label of $0$ indicates that the text does not contain a suggestion.\n",
    "\n",
    "You can find more details about the dataset in Sections 1, 2, 3 and 4 of [SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums\n",
    "](https://aclanthology.org/S19-2151/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "g3PrRwfwGON1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  729k    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  729k  100  729k    0     0   729k      0  0:00:01 --:--:--  0:00:01 1084k\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://raw.githubusercontent.com/pasricha/Subtask-A/master/Data.Assignment2.SemEvalTask9SubtaskA.csv\" > Data.Assignment2.SemEvalTask9SubtaskA.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RohgBTdkHX0Z"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: Reading Data (5 marks)\n",
    "\n",
    "The following cell of code reads the texts and the corresponding labels of suggestion/non-suggestion from the CSV file. The first task is to create training and test sets. Use the final $1000$ rows of the data as a test set and the rest of the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5x0c38rCGk23"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file.\n",
    "df = pd.read_csv('Data.Assignment2.SemEvalTask9SubtaskA.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "# Set seed for reproducibility and shuffle the rows.\n",
    "np.random.seed(888)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "data = [(text, label) for (idx, text, label) in df.values.tolist()]\n",
    "\n",
    "# Create training and test sets.\n",
    "train_texts, train_labels = [], []\n",
    "test_texts, test_labels = [], []\n",
    "\n",
    "#################### EDIT BELOW THIS LINE #########################\n",
    "\n",
    "# your code goes here\n",
    "train_texts, train_labels = [text[0] for text in data[:5100]],[label[1] for label in data[:5100]]\n",
    "test_texts, test_labels = [text[0] for text in data[5100:6100]],[label[1] for label in data[5100:6100]]\n",
    "\n",
    "#################### EDIT ABOVE THIS LINE #########################\n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(test_texts) == len(test_labels) == 1000\n",
    "assert len(train_texts) == len(train_labels) == 5100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x0c38rCGk23"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2: Data Pre-processing (30 Marks)\n",
    "\n",
    "#Explain at least 3 steps that you will perform to preprocess the texts before training a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROipkmu1cnN_"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 300 words.\n",
    "\n",
    "---\n",
    "\n",
    "Before training this classifier I will perform the following steps on the training data:\n",
    "    1. Tokenise - This will seperate a sentence into a list of words and allow operations at a word level.\n",
    "    2. Standardise - This list of words will be converted to lowecase - to treat eg. Version and version the same special characters will also be removed\n",
    "    3. Commonly occurring words (stop words) will be removed - iterating through possibel combinations shows that the top 1100 words are the strongest indicators of class.\n",
    "        Removing NLTKs stopwords was trialled but was removed from the data preprocessing pipeline as it actualy had a negative impact on the clissifiers performance.\n",
    "    4. Remaining words will be lemmatised - so ruin == ruining ==ruined and improve==improvement==improving \n",
    "    5. The preprocessed, tokenised dataset will be converted back into strings to allow CountVector to operate on the new sentences.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2-xXQggaVKh"
   },
   "source": [
    "In the code cell below, write an implementation of the steps you defined above. You are free to use a library such as `nltk` or `sklearn` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "Jb7i3Le4aSYM"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import FreqDist\n",
    "import re\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "wordLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def standardiser(text):\n",
    "    \"\"\"\n",
    "    Takes in a list of texts and removes http, @s and non-alphanumerics\n",
    "    \"\"\"\n",
    "    new_text=[]\n",
    "    for i in text:\n",
    "        lower_case = i.lower()\n",
    "        lower_case = re.sub('[^0-9a-z\\s]+', '', lower_case)\n",
    "        new_text.append(lower_case)\n",
    "    return(new_text)\n",
    "    \n",
    "def tokenizer(text):\n",
    "    return [word_tokenize(i) for i in text]\n",
    "\n",
    "def stop_words(text):\n",
    "    \"\"\"\n",
    "    Using NLTKs stopwords, remove them from all the sentences in the tokenized corpus  \n",
    "    \"\"\"\n",
    "    filtered_corpus =[]\n",
    "    for line in text:\n",
    "        filtered_line = [word for word in line if not word in stopwords]\n",
    "        filtered_corpus.append(filtered_line)\n",
    "    return filtered_corpus\n",
    "    \n",
    "def high_freq_words(text, most_likely, least_likely):\n",
    "    freq_words = FreqDist(word for sentence in text for word in sentence)\n",
    "    target_words = set(w[0] for w in freq_words.most_common(least_likely)) - set(w[0] for w in freq_words.most_common(most_likely))\n",
    "    filtered_corpus =[]\n",
    "    for line in text:\n",
    "        filtered_line = [word for word in line if  word in target_words]\n",
    "        filtered_corpus.append(filtered_line)\n",
    "    return filtered_corpus\n",
    "\n",
    "def lemmatizer(text):\n",
    "    \"\"\"\n",
    "    Using NLTKs WordNetLemmatizer, lemmatize all words in the corpus  \n",
    "    \"\"\"\n",
    "    lemmatized_corpus =[]\n",
    "    for line in text:\n",
    "        lemmatized_line = [wordLemmatizer.lemmatize(word,pos =\"a\") for word in line ]\n",
    "        lemmatized_corpus.append(lemmatized_line)\n",
    "    return lemmatized_corpus\n",
    "\n",
    "\n",
    "# Removing stopwords with NLTKs English dataset had a massive, negative impact on model performance so is commented out but kept in the code to show work done.\n",
    "sd = standardiser(train_texts)\n",
    "tk = tokenizer(sd)\n",
    "#sw = stop_words(tk)\n",
    "\n",
    "# Iterating through various word frequencies and evaluating on training data shows the model F1 scores increase to 70% with a wider vocabulary.\n",
    "# When these models were scored on the test data their performance was down around 50% - this shows significant overfitting by the model as it performs well on training data but\n",
    "# is not able to generalise well to the test dataset. \n",
    "# The High Frequency word count selected is based on a local maximum F1 score found at 0, 1100 on the training data before the model begins overfitting.\n",
    "\n",
    "hfw = high_freq_words(tk, 0, 1100)\n",
    "lem = lemmatizer(hfw)\n",
    "\n",
    "# _prime will be used to denote datasets where a list of tokens has been converted to a string\n",
    "corpus_prime = [\" \".join(word) for word in lem]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IUJunnfXItQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3: Feature Engineering (I) - TF-IDF as features (10 Marks)\n",
    "\n",
    "In the lectures we have seen that raw counts of words and `tf-idf` scores can be useful features for a classification task. Complete the following code cell to create a suggestion detector which uses `tf-idf` scores as features for a Naïve Bayes classifier.\n",
    "\n",
    "After applying your preprocessing steps, use the training data to train the classifier and make predictions on the test set. You **must not** use the test set for training.\n",
    "\n",
    "If everything is implemented correctly, then you should see a single floating point value between 0 and 1 at the end which denotes the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true,
    "id": "3gDsfB8xTGMg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Calculate tf-idf scores for the words in the training set.\n",
    "# ... your code goes here\n",
    "vectoriser = CountVectorizer()\n",
    "word_count_vector = vectoriser.fit_transform(corpus_prime)\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "\n",
    "count_vector=vectoriser.transform(corpus_prime) \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
    "# ... your code goes here\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(tf_idf_vector.toarray(), train_labels)\n",
    "\n",
    "# Predict on the test set.\n",
    "# ... your code goes here\n",
    "X_test_counts = vectoriser.transform(test_texts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "predictions = gnb.predict(X_test_tfidf.toarray())    # save your predictions on the test set into this list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################### DO NOT EDIT BELOW THIS LINE #################\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "  '''\n",
    "  Calculate the accuracy score for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "\n",
    "  assert len(labels) == len(predictions)\n",
    "  \n",
    "  correct = 0\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "    if label == prediction:\n",
    "      correct += 1 \n",
    "  \n",
    "  score = correct / len(labels)\n",
    "  return score\n",
    "\n",
    "# Calculate accuracy score for the classifier using tf-idf features.\n",
    "accuracy(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDx_M2aTIncl"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 4: Evaluation Metrics (15 marks)\n",
    "\n",
    "Why is accuracy not the best measure for evaluating a classifier? Describe an evaluation metric which might work better than accuracy for a classification task such as suggestion detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1evdGdZf66Aw"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 150 words.\n",
    "\n",
    "---\n",
    "\n",
    "Accurracy is not a great metric to use when evaluating a classifier for the following reasons:\n",
    " - it does not capture the bias towards false negatives/false positives eg, for detecting cancer it is underirable to have a system that will bias towards false negatives.\n",
    " - does not work well when classes are not evenly split. In scenarios where one class makes up 95% of the dataset, the model could get a 95% accuracy by predicting that class for all future observations.\n",
    "\n",
    "A more representative metric for evaluating this classifier would be F1 score as it take inputs recall and precision and outputs a balanced measure called the harmonic mean. F1 ranges from 0 to 1 and higher values signify a model that correctly classifies suggestions and does not miss too many obervations. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ozD4SyyRDL3"
   },
   "source": [
    "In the code cell below, write an implementation of the evaluation metric you defined above. You are free to use a library such as `nltk` or `sklearn` for this task, or you can write your own implementation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "UkUX5K0oMhKI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.608433734939759"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(labels, predictions):\n",
    "    '''\n",
    "    Calculate an evaluation score other than accuracy for a given set of predictions and labels.\n",
    "\n",
    "    Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "    Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "    '''\n",
    "\n",
    "    # check that labels and predictions are of same length\n",
    "    assert len(labels) == len(predictions)\n",
    "\n",
    "    score = 0.0\n",
    "\n",
    "    #################### EDIT BELOW THIS LINE #########################\n",
    "\n",
    "    # your code goes here\n",
    "    # This will be my own implimentation of F1 \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        if (label == 1) & (prediction == 1):\n",
    "            TP += 1 \n",
    "        elif (label == 0) & (prediction == 0):\n",
    "            TN += 1\n",
    "        elif (label == 1) & (prediction == 0):\n",
    "            FN += 1\n",
    "        elif (label == 0) & (prediction == 1):\n",
    "            FP += 1\n",
    "\n",
    "    precision = (TP)/(TP+FP)\n",
    "    recall = (TP)/(TP+FN)\n",
    "    score = 2*(precision*recall/(precision + recall))\n",
    "\n",
    "\n",
    "\n",
    "    #################### EDIT ABOVE THIS LINE #########################\n",
    "\n",
    "    return score\n",
    "\n",
    "# Calculate evaluation score based on the metric of your choice\n",
    "# for the classifier trained in Task 3 using tf-idf features.\n",
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22OelF89a27J"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 5: Feature Engineering (II) - Other features (40 Marks)\n",
    "\n",
    "Describe features other than those defined in Task 3 which might improve the performance of your suggestion detector. If these features require any additional pre-processing steps, then define those steps as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQT0m3vG7bNc"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "\n",
    "Other features that can be analysed are:\n",
    "- Out of vocabulary words is the ratio of words not in the baseline language package to the words that are. This might help indicate texts where a user mentions a specific feature or problem, the names of these features are unlikely to be common language and so may stand out for texts with suggestions.\n",
    "\n",
    "- n-grams can show recurring patters across the corpus. This method bundles sentences together in 2s, 3s, 4s, etc. so a sentence like \"I walked to the shops\" in a bigram would look like [\"I walked\", \"walked to\", \"to the\", \"the shop\"] In a model looking at activity by a user, being able to isolate I statements with an activity would be very insightful.\n",
    "\n",
    "- Sentiment analysis could be used to extract texts with a positive and negative sentiment (ignoring neutral texts) as part of a pre-processing step. By feeding these texts into our classifier for training we may improve our ability to correctly classify suggestions.\n",
    "\n",
    "- The use of POS tagging to improve predicitons as it captures the syntactic relationships between words.\n",
    "    The NLTK pos_tag function will output one of 12 tags for each word in the sentence based on the word type - noun, verb etc. which can be used as features to estimate the probability of a suggestion. When fed into a TF-IDF matrix we may be able to see that suggestions contain more of a certain type of word, eg. verbs as suggestions are feedback directing people to do an action.\n",
    "\n",
    "- Latent Semantic Analysis. With the POS tagged corpus there is now a sparse matrix with 32 columns and with dimensionality reduction to a lower level, I could improve classifier performance. I cannot find a satisfying explanaition for how this operates. But given that it was recommended by sci-kit learn after trying PCA truncatedSVD was recommended as does not center data about a mean point before calculating the SVD, enabling it to work with the sparse matrix created by CountVectorizer and TfidfTransformer.\n",
    " \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHYzUHSW7iPx"
   },
   "source": [
    "In the code cell below, write an implementation of the features (and any additional pre-preprocessing steps) you defined above. You are free to use a library such as `nltk` or `sklearn` for this task.\n",
    "\n",
    "After creating your features, use the training data to train a Naïve Bayes classifier and use the test set to evaluate its performance using the metric defined in Task 4. You **must not** use the test set for training.\n",
    "\n",
    "To make sure that your code doesn't take too long to run or use too much memory, you can consider a time limit of 3 minutes and a memory limit of 12GB for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "u9mRku0va8kK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6291486291486291"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your features.\n",
    "# ... your code goes here\n",
    "import itertools\n",
    "import nltk\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def pos_tagger(list_of_tokens):\n",
    "    \"\"\"\n",
    "    Create POS tags for all words in the corpus\n",
    "    \"\"\"\n",
    "    pos_corpus=[]\n",
    "    for word in list_of_tokens:\n",
    "        pos_corpus.append(nltk.pos_tag(word))\n",
    "    return(pos_corpus)\n",
    "\n",
    "def pos_tag_isolate(tagged_corpus):\n",
    "    \"\"\"\n",
    "    The output of pos_tagger is in the format (word, tag) and each tuple is an item in a list for each sentence. I want to extact the pos tag and retain the tags in their respective lists\n",
    "    \"\"\"\n",
    "    pos_corpus=[]\n",
    "    for sentence in tagged_corpus:\n",
    "        line=[]\n",
    "        for word in sentence:\n",
    "            line.append(word[1])\n",
    "        pos_corpus.append(line)\n",
    "    return(pos_corpus)\n",
    "\n",
    "# PART OF SPEECH MODEL\n",
    "# Create a corpus of POS tags for the words in the lemmatized dataset from section 2\n",
    "pos_corpus = pos_tagger(lem)\n",
    "pos_corpus = pos_tag_isolate(pos_corpus)\n",
    "pos_corpus_prime = [\" \".join(word) for word in pos_corpus]\n",
    "\n",
    "\n",
    "# preprocess test data for POS tagged model\n",
    "test_sd = standardiser(test_texts)\n",
    "test_tk = tokenizer(test_sd)\n",
    "pos_X_test = pos_tagger(test_tk)\n",
    "pos_X_test = pos_tag_isolate(pos_X_test)\n",
    "pos_X_test_corpus_prime = [\" \".join(word) for word in pos_X_test]\n",
    "\n",
    "\n",
    "# Train a Naïve Bayes classifier using the features you defined.\n",
    "# ... your code goes here\n",
    "\n",
    "# Instantiate the CountVectoriser and point at the POS tagged corpus\n",
    "# POS tags based vectorisers\n",
    "pos_vectoriser = CountVectorizer()\n",
    "pos_count_vector = vectoriser.fit_transform(pos_corpus_prime) # Input must be a string\n",
    "\n",
    "# Convert above to normalised TF-IDF matrix\n",
    "pos_tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "pos_tfidf_transformer.fit(pos_count_vector)\n",
    "pos_tf_idf_vector=pos_tfidf_transformer.transform(pos_count_vector)\n",
    "\n",
    "\n",
    "\n",
    "X_test_counts = vectoriser.transform(pos_X_test_corpus_prime)\n",
    "pos_X_test_tfidf = pos_tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "# Reduce to a lower dimensision using TruncatedSVD - PCA does not work with sparse matrices\n",
    "# during cross validation n_components was found to maximise the F1 score data at n_components = 15\n",
    "\n",
    "\"\"\"\n",
    "for i in range(2,30,1):\n",
    "    trunc = TruncatedSVD(n_components=i)\n",
    "    principalComponents = trunc.fit_transform(pos_tf_idf_vector)\n",
    "    principalComponents_test = trunc.transform(pos_X_test_tfidf)\n",
    "\n",
    "    skfolds_output=[]\n",
    "    skfolds = StratifiedKFold(n_splits=5)\n",
    "    for train_index, validation_index in skfolds.split(principalComponents, train_labels):\n",
    "        gnb = GaussianNB()\n",
    "        X_train_folds = principalComponents[train_index]\n",
    "        y_train_folds = np.array(train_labels)[train_index]\n",
    "        X_val_folds = principalComponents[validation_index]\n",
    "        y_val_folds = np.array(train_labels)[validation_index]\n",
    "        gnb.fit(X_train_folds, y_train_folds)\n",
    "        y_preds = gnb.predict(X_val_folds)\n",
    "        skfolds_output.append(evaluate(y_val_folds, y_preds))\n",
    "    print(f\"Mean F1 for above model is: {np.mean(skfolds_output)} at n= {i}\")\n",
    "    \n",
    "    \"\"\"\n",
    "trunc = TruncatedSVD(n_components=15)\n",
    "SVDComponents = trunc.fit_transform(pos_tf_idf_vector)\n",
    "SVDComponents_test = trunc.transform(pos_X_test_tfidf)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
    "# ... your code goes here\n",
    "\n",
    "gnb.fit(SVDComponents, train_labels)\n",
    "\n",
    "# Predict on the test set.\n",
    "# ... your code goes here\n",
    "\n",
    "y_predictions = gnb.predict(SVDComponents_test)\n",
    "\n",
    "\n",
    "# Evaluate on the test set.\n",
    "# ... your code goes here\n",
    "evaluate(test_labels, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
