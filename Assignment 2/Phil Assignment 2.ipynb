{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6saNLUFqHb"
   },
   "source": [
    "# Assignment 2 - CT5120/CT5146\n",
    "\n",
    "### Instructions:\n",
    "- Complete all the tasks below and upload your submission as a Python notebook on Blackboard with the filename “`StudentID_Lastname.ipynb`” before **23:59** on **October 24, 2021**.\n",
    "- This is an individual assignment, you **must not** work with other students to complete this assessment.\n",
    "- The assignment is worth $100$ marks and constitutes 19% of the final grade. The breakdown of the marking scheme for each task is as follows:\n",
    "\n",
    "| Task | Marks for write-up | Marks for code | Total Marks |\n",
    "| :--- | :----------------- | :------------- | :---------- |\n",
    "| 1    |                  - |              5 |           5 |\n",
    "| 2    |                 15 |             15 |          30 |\n",
    "| 3    |                  - |             10 |          10 |\n",
    "| 4    |                 10 |              5 |          15 |\n",
    "| 5    |                 15 |             25 |          40 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCWSEtNeGMsN"
   },
   "source": [
    "---\n",
    "\n",
    "This assignment involves tasks for feature engineering, training and evaluating a classifier for suggestion detection. You will work with the data from SemEval-2019 Task 9 subtask A to classify whether a piece of text contains a suggestion or not. \n",
    "\n",
    "\n",
    "Download `Data.Assignment2.SemEvalTask9SubtaskA.csv` from Blackboard or uncomment the code cell below to get the data as a comma-separated values (CSV) file. The CSV file contains a header row followed by 6,100 rows spread across 3 columns of data. Each row of data contains a unique id, a piece of text and a label assigned by an annotator. A label of $1$ indicates that the given text contains a suggestion while a label of $0$ indicates that the text does not contain a suggestion.\n",
    "\n",
    "You can find more details about the dataset in Sections 1, 2, 3 and 4 of [SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums\n",
    "](https://aclanthology.org/S19-2151/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "g3PrRwfwGON1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 87  729k   87  637k    0     0   637k      0  0:00:01 --:--:--  0:00:01  949k\n",
      "100  729k  100  729k    0     0   729k      0  0:00:01 --:--:--  0:00:01 1061k\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://raw.githubusercontent.com/pasricha/Subtask-A/master/Data.Assignment2.SemEvalTask9SubtaskA.csv\" > Data.Assignment2.SemEvalTask9SubtaskA.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RohgBTdkHX0Z"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: Reading Data (5 marks)\n",
    "\n",
    "The following cell of code reads the texts and the corresponding labels of suggestion/non-suggestion from the CSV file. The first task is to create training and test sets. Use the final $1000$ rows of the data as a test set and the rest of the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5x0c38rCGk23"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file.\n",
    "df = pd.read_csv('Data.Assignment2.SemEvalTask9SubtaskA.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "# Set seed for reproducibility and shuffle the rows.\n",
    "np.random.seed(888)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "data = [(text, label) for (idx, text, label) in df.values.tolist()]\n",
    "\n",
    "# Create training and test sets.\n",
    "train_texts, train_labels = [], []\n",
    "test_texts, test_labels = [], []\n",
    "\n",
    "#################### EDIT BELOW THIS LINE #########################\n",
    "\n",
    "# your code goes here\n",
    "train_texts, train_labels = [text[0] for text in data[:5100]],[label[1] for label in data[:5100]]\n",
    "test_texts, test_labels = [text[0] for text in data[5100:6100]],[label[1] for label in data[5100:6100]]\n",
    "\n",
    "#################### EDIT ABOVE THIS LINE #########################\n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(test_texts) == len(test_labels) == 1000\n",
    "assert len(train_texts) == len(train_labels) == 5100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x0c38rCGk23"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2: Data Pre-processing (30 Marks)\n",
    "\n",
    "#Explain at least 3 steps that you will perform to preprocess the texts before training a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROipkmu1cnN_"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 300 words.\n",
    "\n",
    "---\n",
    "\n",
    "Before training this classifier I will perform the following steps on the training data:\n",
    "    1. Tokenise - This will seperate a sentence into a list of words and allow operations at a word level.\n",
    "    2. Standardise - This list of words will be converted to lowecase - to treat eg. Version and version the same special characters will also be removed\n",
    "    3. Commonly occurring words (stop words) will be removed - this includes the top 50/100 \n",
    "    4. Remaining words will be lemmatised - so ruin == ruining ==ruined and improve==improvement==improving \n",
    "    \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2-xXQggaVKh"
   },
   "source": [
    "In the code cell below, write an implementation of the steps you defined above. You are free to use a library such as `nltk` or `sklearn` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Jb7i3Le4aSYM"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import FreqDist\n",
    "import re\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "wordLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def standardiser(text):\n",
    "    \"\"\"\n",
    "    Takes in a list of texts and removes http, @s and non-alphanumerics\n",
    "    \"\"\"\n",
    "    new_text=[]\n",
    "    for i in text:\n",
    "        lower_case = i.lower()\n",
    "        lower_case = re.sub('[^0-9a-zA-Z\\s]+', '', lower_case)\n",
    "        new_text.append(lower_case)\n",
    "    return(new_text)\n",
    "    \n",
    "def tokenizer(text):\n",
    "    return [word_tokenize(i) for i in text]\n",
    "\n",
    "def stop_words(text):\n",
    "    \"\"\"\n",
    "    Using NLTKs stopwords, remove them from all the sentences in the tokenized corpus  \n",
    "    \"\"\"\n",
    "    filtered_corpus =[]\n",
    "    for line in text:\n",
    "        filtered_line = [word for word in line if not word in stopwords]\n",
    "        filtered_corpus.append(filtered_line)\n",
    "    return filtered_corpus\n",
    "    \n",
    "def lemmatizer(text):\n",
    "    \"\"\"\n",
    "    Using NLTKs WordNetLemmatizer, lemmatize all words in the corpus  \n",
    "    \"\"\"\n",
    "    lemmatized_corpus =[]\n",
    "    for line in text:\n",
    "        lemmatized_line = [wordLemmatizer.lemmatize(word,pos =\"a\") for word in line ]\n",
    "        lemmatized_corpus.append(lemmatized_line)\n",
    "    return lemmatized_corpus\n",
    "\n",
    "\n",
    "sd = standardiser(train_texts)\n",
    "tk = tokenizer(sd)\n",
    "sw = stop_words(tk)\n",
    "\n",
    "lem = lemmatizer(sw)\n",
    "corpus_prime = [\" \".join(word) for word in lem]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IUJunnfXItQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3: Feature Engineering (I) - TF-IDF as features (10 Marks)\n",
    "\n",
    "In the lectures we have seen that raw counts of words and `tf-idf` scores can be useful features for a classification task. Complete the following code cell to create a suggestion detector which uses `tf-idf` scores as features for a Naïve Bayes classifier.\n",
    "\n",
    "After applying your preprocessing steps, use the training data to train the classifier and make predictions on the test set. You **must not** use the test set for training.\n",
    "\n",
    "If everything is implemented correctly, then you should see a single floating point value between 0 and 1 at the end which denotes the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "3gDsfB8xTGMg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.566"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Calculate tf-idf scores for the words in the training set.\n",
    "# ... your code goes here\n",
    "vectoriser = CountVectorizer()\n",
    "word_count_vector = vectoriser.fit_transform(corpus_prime)\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "\n",
    "count_vector=vectoriser.transform(corpus_prime) \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
    "# ... your code goes here\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(tf_idf_vector.toarray(), train_labels)\n",
    "\n",
    "# Predict on the test set.\n",
    "# ... your code goes here\n",
    "X_test_counts = vectoriser.transform(test_texts)\n",
    "predictions = gnb.predict(X_test_counts.toarray())    # save your predictions on the test set into this list\n",
    "\n",
    "\n",
    "#################### DO NOT EDIT BELOW THIS LINE #################\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "  '''\n",
    "  Calculate the accuracy score for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "\n",
    "  assert len(labels) == len(predictions)\n",
    "  \n",
    "  correct = 0\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "    if label == prediction:\n",
    "      correct += 1 \n",
    "  \n",
    "  score = correct / len(labels)\n",
    "  return score\n",
    "\n",
    "# Calculate accuracy score for the classifier using tf-idf features.\n",
    "accuracy(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDx_M2aTIncl"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 4: Evaluation Metrics (15 marks)\n",
    "\n",
    "Why is accuracy not the best measure for evaluating a classifier? Describe an evaluation metric which might work better than accuracy for a classification task such as suggestion detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1evdGdZf66Aw"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 150 words.\n",
    "\n",
    "---\n",
    "\n",
    "Accurracy is not a great metric to use when evaluating a classifier for the following reasons:\n",
    " - it does not capture the bias towards false negatives/false positives eg, for detecting cancer it is underirable to have a system that will bias towards false negatives.\n",
    " - does not work well when classes are not evenly split. In scenarios where one class makes up 95% of the dataset, the model could get a 95% accuracy by predicting that class for all future observations.\n",
    "\n",
    "A more representative metric for evaluating this classifier would be F1 score as it take inputs recall and precision and outputs a balanced measure called the harmonic mean. F1 ranges from 0 to 1 and higher values signify a model that correctly classifies suggestions and does not miss too many obervations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ozD4SyyRDL3"
   },
   "source": [
    "In the code cell below, write an implementation of the evaluation metric you defined above. You are free to use a library such as `nltk` or `sklearn` for this task, or you can write your own implementation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UkUX5K0oMhKI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5272331154684096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(labels, predictions):\n",
    "    '''\n",
    "    Calculate an evaluation score other than accuracy for a given set of predictions and labels.\n",
    "\n",
    "    Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "    Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "    '''\n",
    "\n",
    "    # check that labels and predictions are of same length\n",
    "    assert len(labels) == len(predictions)\n",
    "\n",
    "    score = 0.0\n",
    "\n",
    "    #################### EDIT BELOW THIS LINE #########################\n",
    "\n",
    "    # your code goes here\n",
    "    # This will be my own implimentation of F1 \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        if (label == 1) & (prediction == 1):\n",
    "            TP += 1 \n",
    "        elif (label == 0) & (prediction == 0):\n",
    "            TN += 1\n",
    "        elif (label == 1) & (prediction == 0):\n",
    "            FN += 1\n",
    "        elif (label == 0) & (prediction == 1):\n",
    "            FP += 1\n",
    "\n",
    "    precision = (TP)/(TP+FP)\n",
    "    recall = (TP)/(TP+FN)\n",
    "    score = 2*(precision*recall/(precision + recall))\n",
    "\n",
    "\n",
    "\n",
    "    #################### EDIT ABOVE THIS LINE #########################\n",
    "\n",
    "    return score\n",
    "\n",
    "# Calculate evaluation score based on the metric of your choice\n",
    "# for the classifier trained in Task 3 using tf-idf features.\n",
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22OelF89a27J"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 5: Feature Engineering (II) - Other features (40 Marks)\n",
    "\n",
    "Describe features other than those defined in Task 3 which might improve the performance of your suggestion detector. If these features require any additional pre-processing steps, then define those steps as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQT0m3vG7bNc"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "\n",
    "Other preprocessing steps that can be applied:\n",
    " - Frequency based stop word removal. Count the term frequency and remove the top N occurring words - iterating for upper and lower cut-offs to maximise F1\n",
    " - Grouping words in a vector space using Word2Vec to group words with similar meanings together\n",
    " - Sentence level representation by averaging the Word2Vec scores for all words in a sentence.\n",
    " \n",
    " \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHYzUHSW7iPx"
   },
   "source": [
    "In the code cell below, write an implementation of the features (and any additional pre-preprocessing steps) you defined above. You are free to use a library such as `nltk` or `sklearn` for this task.\n",
    "\n",
    "After creating your features, use the training data to train a Naïve Bayes classifier and use the test set to evaluate its performance using the metric defined in Task 4. You **must not** use the test set for training.\n",
    "\n",
    "To make sure that your code doesn't take too long to run or use too much memory, you can consider a time limit of 3 minutes and a memory limit of 12GB for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "u9mRku0va8kK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 500 0.5757225433526012\n",
      "0 1000 0.5949656750572083\n",
      "0 1500 0.5900552486187846\n",
      "0 2000 0.574537540805223\n",
      "0 2500 0.5849889624724062\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8ae4cdf8f959>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0msd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardiser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0msw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mhfw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhigh_freq_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhighest_rank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowest_rank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ee599af87bae>\u001b[0m in \u001b[0;36mtokenizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ee599af87bae>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     return [token for sent in sentences\n\u001b[0m\u001b[0;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m--> 130\u001b[1;33m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# Handles parentheses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36m_subx\u001b[1;34m(pattern, template)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;31m# internal: pattern.sub/subn implementation helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m     \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# literal replacement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create your features.\n",
    "# ... your code goes here\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import itertools\n",
    "def high_freq_words(text, most_likely, least_likely):\n",
    "    freq_words = FreqDist(word for sentence in text for word in sentence)\n",
    "    target_words = set(w[0] for w in freq_words.most_common(least_likely)) - set(w[0] for w in freq_words.most_common(most_likely))\n",
    "    filtered_corpus =[]\n",
    "    for line in text:\n",
    "        filtered_line = [word for word in line if  word in target_words]\n",
    "        filtered_corpus.append(filtered_line)\n",
    "    return filtered_corpus\n",
    " \n",
    "for highest_rank, lowest_rank in itertools.product(range(0,101,10), range(500,5001,500)):\n",
    "\n",
    "    sd = standardiser(train_texts)\n",
    "    tk = tokenizer(sd)\n",
    "    sw = stop_words(tk)\n",
    "    hfw = high_freq_words(sw, highest_rank, lowest_rank)\n",
    "    lem = lemmatizer(hfw)\n",
    "    corpus_prime = [\" \".join(word) for word in lem]\n",
    "\n",
    "\n",
    "    # Train a Naïve Bayes classifier using the features you defined.\n",
    "    # ... your code goes here\n",
    "    vectoriser = CountVectorizer()\n",
    "    word_count_vector = vectoriser.fit_transform(corpus_prime)\n",
    "\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "    count_vector=vectoriser.transform(corpus_prime) \n",
    "    tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "\n",
    "    # Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
    "    # ... your code goes here\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(tf_idf_vector.toarray(), train_labels)\n",
    "\n",
    "    # Predict on the test set.\n",
    "    # ... your code goes here\n",
    "    X_test_counts = vectoriser.transform(test_texts)\n",
    "    predictions = gnb.predict(X_test_counts.toarray())\n",
    "\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    # ... your code goes here\n",
    "    print(highest_rank, lowest_rank, evaluate(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
