{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHfQMjDnlACs"
   },
   "source": [
    "# Assignment 3 - CT5120/CT5146\n",
    "\n",
    "### Instructions:\n",
    "- Complete all the tasks below and upload your submission as a Python notebook on Blackboard with the filename “`StudentID_Lastname.ipynb`” before **23:59** on **December 31, 2021**. Please note that there will be no further extensions to this deadline and we highly encourage you to submit this assignment before Semester 1 exams.\n",
    "- This is an individual assignment, you **must not** work with other students to complete this assessment.\n",
    "- The assignment is worth $100$ marks and constitutes 19% of the final grade. The breakdown of the marking scheme for each task is as follows:\n",
    "\n",
    "|           | Task | Marks |\n",
    "| :---      | :-----| -----:|\n",
    "| Task 1    | Pre-processing |   15 |\n",
    "| Task 2    | Named Entity Recognition |    10 |\n",
    "| Task 3    | Information / Relation Extraction (I) | 30 |\n",
    "| Task 4    | Information / Relation Extraction (II) | 15 |\n",
    "| Task 5    | Combining information in the output   | 5 |\n",
    "| Task 6    | Evaluation (I) | 15 |\n",
    "| Task 7    | Evaluation (II) | 10 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEpjx2qKnyBl"
   },
   "source": [
    "---\n",
    "\n",
    "## Information Extraction and Relation Extraction\n",
    "\n",
    "In the following tasks you will write code to perform **_information extraction_** and **_relation extraction_** across a collection of documents in `movies.zip`.\n",
    "\n",
    "The zip archive contains 100 files, out of which 50 are plaintext documents and other 50 contain data structured as JSON.\n",
    "Each plaintext document contains a text description of a movie taken from the English version of Wikipedia, while each JSON document contains *gold-standard* labels (also called *reference* labels) stored as key-value pairs for the entities and relations for each document.\n",
    "\n",
    "You are only allowed to use the given documents and labels and **must not** use any other external sources of data for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64euw8hXygah"
   },
   "source": [
    "---\n",
    "\n",
    "Download and unarchive `movies.zip` from Blackboard and place it in the same location as this notebook or uncomment the code cell below to get the data in a directory called `movies` and also place it automatically in the same location as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OXzoZVNZyevs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-f was unexpected at this time.\n",
      "-d was unexpected at this time.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!if test -f \"movies.zip\"; then rm \"movies.zip\"; fi\n",
    "!if test -d \"movies/\"; then rm -rf \"movies/\"; fi\n",
    "!wget \"https://drive.google.com/uc?export=download&id=1L6NcSGkubNJaL6xSnYEZZKSrlyXq1AbB\" -O \"movies.zip\"\n",
    "!unzip \"movies.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm1emJILzF5K"
   },
   "source": [
    "---\n",
    "\n",
    "## Reading Data\n",
    "\n",
    "Place the unzipped `movies` directory in the same location as this notebook and run the following code cell to read the plaintext and JSON documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "P5hzct-HzUvJ"
   },
   "outputs": [],
   "source": [
    "######### DO NOT EDIT THIS CELL #########\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "documents = []   # store the text documents as a list of strings\n",
    "labels = []      # store the gold-standard labels as a list of dictionaries\n",
    "\n",
    "for idx in range(50):\n",
    "  with open(os.path.join('movies', str(idx+1).zfill(2) + '.doc.txt') , encoding='utf8') as f:\n",
    "    doc = f.read().strip()\n",
    "  with open(os.path.join('movies', str(idx+1).zfill(2) + '.info.json'), encoding='utf8') as f:\n",
    "    label = json.load(f)\n",
    "\n",
    "  documents.append(doc)\n",
    "  labels.append(label)\n",
    "\n",
    "assert len(documents) == 50\n",
    "assert len(labels) == 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnmnLhDyj2eG"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nPCKvyYFj0zG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the libraries which might be useful\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('all', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "899-kd7LmlFp"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: Document Pre-processing (15 Marks)\n",
    "Write a function that takes a document and returns a list of sentences with part-of-speech tags.\n",
    "\n",
    "The expected output is a list of tagged sentences where each tagged sentence is a list containing `(token, tag)` pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dB8R43AklZxO"
   },
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    '''\n",
    "    Return a list of sentences tagged with part-of-speech tags for the given document.\n",
    "    '''\n",
    "    tagged_sentences = []\n",
    "\n",
    "  # your code goes here\n",
    "  # ...\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    for sentence in sentences:\n",
    "        tagged_sentences.append(nltk.pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KJLD-AMMvdq"
   },
   "source": [
    "Run the cell below to check if the output is formatted correctly.\n",
    "\n",
    "Expected output: `[('It', 'PRP'), ('received', 'VBD'), ('ten', 'JJ'), ('Oscar', 'NNP'), ('nominations', 'NNS'), ('(', '('), ('including', 'VBG'), ('Best', 'NNP'), ('Picture', 'NN'), (')', ')'), (',', ','), ('winning', 'VBG'), ('seven', 'CD'), ('.', '.')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9sEQYa3TBDYE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('received', 'VBD'),\n",
       " ('ten', 'JJ'),\n",
       " ('Oscar', 'NNP'),\n",
       " ('nominations', 'NNS'),\n",
       " ('(', '('),\n",
       " ('including', 'VBG'),\n",
       " ('Best', 'NNP'),\n",
       " ('Picture', 'NN'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('winning', 'VBG'),\n",
       " ('seven', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check output for Task 1\n",
    "ie_preprocess(documents[0])[-10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgEPCLXmq8bC"
   },
   "source": [
    "## Task 2: Named Entity Recognition (10 Marks)\n",
    "\n",
    "Write a function that returns a list of all the named entities in a given document. The document here is structured as a list of sentences and tagged with part-of-speech tags.\n",
    "\n",
    "Hint: Set `binary = True` while calling the `ne_chunk` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qHKjz5TKp5uM"
   },
   "outputs": [],
   "source": [
    "def find_named_entities(tagged_document):\n",
    "    '''Return a list of all the named entities in the given tagged document.'''\n",
    "  \n",
    "    named_entities = []\n",
    "\n",
    "    # your code goes here\n",
    "    # ...\n",
    "    tree = nltk.ne_chunk(tagged_document[0], binary=True)\n",
    "    \n",
    "    for subtree in tree.subtrees():\n",
    "        entity = \"\"\n",
    "        for leaf in subtree.leaves():\n",
    "            entity = entity+leaf[0]+\" \"\n",
    "        named_entities.append(entity.strip())\n",
    "    \n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvZNyV-ENDZc"
   },
   "source": [
    "Run the cell below to check if the output is formatted correctly.\n",
    "\n",
    "The output values might not match exactly, but should look similar to: `['Star Wars', 'Star Wars', 'New Hope', 'American', 'George Lucas', 'Lucasfilm', ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lnlqsKg7sk29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Star Wars',\n",
       " 'Star Wars',\n",
       " 'New Hope',\n",
       " 'American',\n",
       " 'George Lucas',\n",
       " 'Lucasfilm',\n",
       " 'Century Fox']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check output for Task 2\n",
    "tagged_document = ie_preprocess(documents[0]) # pre-process the first document\n",
    "find_named_entities(tagged_document)[1:10]     # display the first 10 named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmpuu0yvwI_X"
   },
   "source": [
    "## Task 3: Information / Relation Extraction (I) (30 Marks)\n",
    "\n",
    "Choose any **three** relations out of the following and write functions to extract them from a given document.\n",
    "\n",
    "* **Title**\n",
    "* **Language**\n",
    "* **Starring**\n",
    "* **Release date**\n",
    "* **Cinematography**\n",
    "* **Dialogue by**\n",
    "* **Directed by**\n",
    "* **Edited by**\n",
    "* **Music by**\n",
    "* **Narrated by**\n",
    "* **Produced by**\n",
    "* **Screenplay by**\n",
    "* **Story by**\n",
    "* **Written by**\n",
    "* **Production companies**\n",
    "* **Distribution companies**\n",
    "* **Budget**\n",
    "* **Box office**\n",
    "\n",
    "\n",
    "The functions you define here must take as input a string called `document` and return the information/relation extracted as a list. You can explain your approach with comments along with your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "id": "Yw8YQAr-wwFM"
   },
   "outputs": [],
   "source": [
    "# relation 1 - your code goes here\n",
    "def extract_budget(document):\n",
    "    try:\n",
    "        dollar_refs = (re.findall(r'[gG]ross\\w{0-8}\\s\\$\\w*\\s[b,m]*\\w*|\\$\\w*\\s[bm]*\\w*| [bB]ox\\s[oO]ffice\\w{0-8}\\s\\$\\w*\\s[b,m]*\\w*|\\$\\w*\\s[bm]*\\w*', document))\n",
    "    \n",
    "    # Typically these documents have a budget and a box office gross income and hopefully the gross is greater than the budget, so return the lowest million/billion dollar entry.\n",
    "        return[max(dollar_refs)]\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "id": "16p4KMlVyQU3"
   },
   "outputs": [],
   "source": [
    "# relation 2 - your code goes here\n",
    "\"\"\"\n",
    "Date not being used as the exact dates are not called out in some labels\n",
    "\n",
    "def extract_releaseDate(document):\n",
    "    # Extract text in format dd/mm/yyyy after keyword 'on' (released on ... ) or any year in format YYYY following keyword 'a' (this is a 2018 movie...) \n",
    "    date_refs = re.findall(r'[on]*\\s*[0-9]{1,2}\\s[a-z,A-Z]*\\s[0-9]{2,4}|([a]\\s)([0-9]{4})', document)[0][1]\n",
    "    return [date_refs]\"\"\"\n",
    "\n",
    "def extract_director(document):\n",
    "    try:\n",
    "        # Extract the text following 'produced by', can be a single studio or a FirstName Surname producer\n",
    "        director_refs = re.findall(r'([dD]irected\\sby\\s)([A-Z]\\w*) | ([dD]irected\\sby\\s)([A-Z\\.]\\w*\\s[A-Z\\.]\\w*)| ([dD]irected\\sby\\s)([A-Z]\\.\\w*\\s[A-Z]\\.*\\w*\\s[A-Z\\.]\\w*)', document)[0]\n",
    "    except:\n",
    "        director_refs='None'\n",
    "    #pdb.set_trace()\n",
    "    if len(director_refs[1])>1:\n",
    "        return [director_refs[1]]\n",
    "    elif len(director_refs[3])>1:\n",
    "        return [director_refs[3]]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "id": "x-LRnjy810ZR"
   },
   "outputs": [],
   "source": [
    "# relation 3 - your code goes here\n",
    "def extract_title(document):\n",
    "    try:\n",
    "        # Extract the title by taking text before 'is a' shows up or before the first ( as is the case with a few Star Ward movies\n",
    "        title_ref = re.split(' \\(| is a',re.match(r\"([a-z,A-Z, 0-9,\\:, \\, \\-']*)\\sis a|([a-z,A-Z,0-9, \\: \\( ]*)\\s\\(*\\w*\", document)[0])[0]\n",
    "    except:\n",
    "        title_ref=None\n",
    "    return [title_ref]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuJmr-eKvrQ3"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 4: Information / Relation Extraction (II)  (15 Marks)\n",
    "\n",
    "Identify one other relation of your choice, besides the ones mentioned in the previous task, and write a function to extract it. \n",
    "\n",
    "The function you define here must take as input a string called `document` and return the information/relations extracted as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "id": "kncUM3pHvyAT"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "def extract_producer(document):\n",
    "    try:\n",
    "        # Extract the text following 'produced by', can be a single studio or a FirstName Surname producer\n",
    "        producer_refs = re.findall(r'([pP]roduced\\sby\\s)([A-Z]\\w*) | ([pP]roduced\\sby\\s)([A-Z]\\w*\\s[A-Z]\\w*)', document)[0]\n",
    "    except:\n",
    "        producer_refs='None'\n",
    "    #pdb.set_trace()\n",
    "    if len(producer_refs[1])>1:\n",
    "        return [producer_refs[1]]\n",
    "    elif len(producer_refs[3])>1:\n",
    "        return [producer_refs[3]]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "[extract_producer(i) for i in documents]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIfQCd_Y1x5B"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 5: Combining information in the output (5 Marks)\n",
    "\n",
    "Edit the function below to return a Python dictionary with the outputs from the functions defined in tasks $3 - 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "id": "BE_8ptxp-1y4"
   },
   "outputs": [],
   "source": [
    "def extract_info(document):\n",
    "    '''Extract information and relations from a given document.'''\n",
    "\n",
    "  # Edit the output dict below and assign the values to keys by \n",
    "  # calling the appropriate functions from Tasks 3 and 4.\n",
    "  \n",
    "  # You can delete the keys for which you do not perform extraction in Task 3.\n",
    "\n",
    "    output = {\n",
    "    ##### EDIT BELOW THIS LINE #####\n",
    "    \n",
    "    # For the relations you extract in Task 3, \n",
    "    # save the output in the appropriate key and delete rest of the keys.\n",
    "    \n",
    "    \"Title\": extract_title(document),\n",
    "    #Release date\": extract_releaseDate(document),\n",
    "    \"Box office\": extract_budget(document),\n",
    "    \"Directed by\":extract_director(document),\n",
    "\n",
    "    # save the output from Task 4 here\n",
    "    \"Produced by\": extract_producer(document),\n",
    "\n",
    "    ##### EDIT ABOVE THIS LINE #####\n",
    "    }\n",
    "    return output\n",
    "\n",
    "\n",
    "# check output for the first document\n",
    "#extract_info(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = [extract_info(i) for i in documents]\n",
    "#predictions = [{'Box office':extract_budget(i)} for i in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHv5pRQ7BKJo"
   },
   "source": [
    "The output from the cell above should look something like the dictionary shown below. Overall values might be different, based on what four items you choose to extract in Tasks 3 and 4, but the structure should be similar.\n",
    "\n",
    "For example, if you choose to extract **Starring**, **Release Date**, **Box office**, and **Directed by**, then the output should look something like this for the first document:\n",
    "\n",
    "```javascript\n",
    "{\n",
    "  'Box office': ['$775 million'],\n",
    "  'Directed by': ['George Lucas'],\n",
    "  'Release date': ['May 25, 1977'],\n",
    "  'Starring': ['Mark Hamill', 'Harrison Ford', 'Carrie Fisher', \n",
    "               'Peter Cushing', 'David Prowse', 'James Earl Jones', ],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDMhFQq4fBnf"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 6: Evaluation (I) (15 Marks)\n",
    "\n",
    "Write a function to evaluate the performance of Task $3$ using **Precision**, **Recall** and **F1** scores. Use the gold-standard labels provided in the JSON files to calculate these values.\n",
    "\n",
    "Please note that not all the information / relations mentioned in Task $3$ have associated labels for each and every movie in the JSON documents, i.e., some JSON documents will have certain keys-value pairs missing. For example, we have labels for *Budget* in 46 out of the 50 movies and in the remaining 4 documents, you will find that the key `Budget` is omitted from the JSON.\n",
    " \n",
    "Also keep in mind that we will further run this evaluation on a hidden test set containing similar movie descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "id": "OvJ9OhnDe7ML"
   },
   "outputs": [],
   "source": [
    "def evaluate(labels, predictions):\n",
    "    '''\n",
    "    Evaluate the performance of relation extraction \n",
    "    using Precision, Recall, and F1 scores.\n",
    "\n",
    "    Args:\n",
    "    labels: A list containing gold-standard labels\n",
    "    predictions: A list containing information extracted from documents\n",
    "    Returns:\n",
    "    scores: A dictionary containing Precision, Recall and F1 scores \n",
    "            for the information/relations extracted in Task 3.\n",
    "    '''\n",
    "\n",
    "    assert len(predictions) == len(labels)\n",
    "    # Create a dictionary of the relevent details so dictionarys can be compared between extrasions and labels\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    # The list of predicitons needs to be unpacked and any extractions that failed and given a None value need to be removed.\n",
    "    pred_list = []\n",
    "    for l in predictions, :\n",
    "        for d in l:\n",
    "            for key in d:\n",
    "                if d[key] is not None:\n",
    "                    pred_list.append(d[key])\n",
    "\n",
    "\n",
    "\n",
    "    # gs is multiplied by 4 as there are 4 features being looked at. If these were left as normal\n",
    "    # lengths of the labels then cex would exceed the gs counts because it is matching across 4 features, skewing precision, recall and f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gs = len([i for i in labels])*4\n",
    "    ex = len(pred_list)\n",
    "    cex=0\n",
    "\n",
    "    # Not all the information needed to get a full match is in the datasets so I'm using 'in' to match on any \n",
    "    # of the labelled values as some movies have multiple producers not \n",
    "    # mentioned so an exact == would give a negative result although according to the data this is correct\n",
    "\n",
    "    # Go thorugh all the predicion items\n",
    "    for index, item in enumerate(predictions):\n",
    "        try:\n",
    "            for key  in item:\n",
    "                # For each dictionary check each key and the same key in the labelled dataset. If the prediciton is in the label then increment cex by 1\n",
    "                if item[key][0] in labels[index][key][0]:\n",
    "                    cex+=1\n",
    "                else:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "    precision = cex/ex\n",
    "    recall = cex/gs\n",
    "    if cex>0: \n",
    "        f1 = (2*precision*recall)/(precision+recall)\n",
    "    else:\n",
    "        f1 = 0           \n",
    "\n",
    "    scores = {\n",
    "      'precision': precision, 'recall': recall, 'f1': f1\n",
    "    }\n",
    "\n",
    "  # calculate the precision, recall and f1 score over the information fields \n",
    "  # corresponding to Task 3 and store the result in the `scores` dict.\n",
    "\n",
    "  # your code goes here\n",
    "  # ...\n",
    "\n",
    "\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5290322580645161, 'recall': 0.41, 'f1': 0.4619718309859155}"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA39EbBCfRu6"
   },
   "source": [
    "---\n",
    "Run the cell below to calculate and display the evaluation scores for the 50 documents in `movies.zip`.\n",
    "\n",
    "You can consider the following as a baseline score. Your aim should be to score higher or atleast get as close as possible to these values.\n",
    "\n",
    "| Precision | Recall | F1    |\n",
    "| :---:     | :---:  | :---: |\n",
    "| 0.5       | 0.25   | 0.333 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "id": "CRxOd4dIfRu-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.529032</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.461972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision  recall        f1\n",
       "0   0.529032    0.41  0.461972"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "# calculate evaluation score across all the 50 documents\n",
    "extracted_infos = []\n",
    "for document in documents:\n",
    "  extracted_infos.append(extract_info(document))\n",
    "\n",
    "scores = evaluate(labels, extracted_infos)\n",
    "\n",
    "pd.DataFrame([scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agNQPVqG5aoS"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 7: Evaluation (II) (10 Marks)\n",
    "\n",
    "Describe **two** challenges you encountered above or might encounter in the evaluation of *information extraction* or *relation extraction* tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfdKI5J-fF1g"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 100 words. No coding is required for this task.\n",
    "\n",
    "---\n",
    "Difficulties encountered were: <br>\n",
    "1 - Extracting the correct relation. In the given datasets, there were often many different name formats and sequence of words before a relationship. In the case of directors, J. J. Abrams was causing difficulties because of the repeated J. and that is is a three word, multiword expression and preventing extra words being extracted with 2 word names. There were examples where 'directed by' was followed by 'produced by' and a movie with what appeared to be two brothers named together 'Joe and Larry Smith' that I couldnt build a rule for. Extracting dates was also difficult due to the amoun tof possibilities, dd/mm/yyyy, dd/Jan/YY etc. Often the extraction I detected was just the year\n",
    "\n",
    "2- The gold standard contained much more information than was available in the text documents. In the case of Star Wars, the only date given in the document is the year, 1977 but the gold standard had the day and month with some movies also including release dates for other countries. The same is true for some of the other relationships like producers with multiple names in the labelled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
